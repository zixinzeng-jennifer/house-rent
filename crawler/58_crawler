import requests
import sys,io
import numpy as np
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from bs4 import BeautifulSoup
import base64
from fontTools.ttLib import TTFont
from io import BytesIO
import time
import codecs
import re
fontPath = 'decode.ttf'
all_data=[]
geo={}
#geo['北京']=['朝阳','海淀','昌平','丰台','大兴','通州','房山','顺义','西城','东城','石景山','怀柔','门头沟']
#geo['上海']=['浦东','闵行','宝山','嘉定', '徐汇', '静安', '普陀','杨浦','奉贤','黄浦','虹口','长宁']
geo['深圳']=['宝安','南山','龙华','福田','龙岗','罗湖','布吉','光明','盐田']
url_dict={}
url_dict['朝阳']='https://bj.58.com/chaoyang/zufang/'
url_dict['海淀']='https://bj.58.com/haidian/zufang/'
url_dict['昌平']='https://bj.58.com/changping/zufang/'
url_dict['丰台']='https://bj.58.com/fengtai/zufang/'
url_dict['大兴']='https://bj.58.com/daxing/zufang/'
url_dict['通州']='https://bj.58.com/tongzhouqu/zufang/'
url_dict['房山']='https://bj.58.com/fangshan/zufang/'
url_dict['顺义']='https://bj.58.com/shunyi/zufang/'
url_dict['西城']='https://bj.58.com/xicheng/zufang/'
url_dict['东城']='https://bj.58.com/dongcheng/zufang/'
url_dict['石景山']='https://bj.58.com/shijingshan/zufang/'
url_dict['怀柔']='https://bj.58.com/huairou/zufang/'
url_dict['门头沟']='https://bj.58.com/mentougou/zufang/'
url_dict['浦东']='https://sh.58.com/pudongxinqu/zufang/'
url_dict['闵行']='https://sh.58.com/minxing/zufang/'
url_dict['宝山']='https://sh.58.com/baoshan/zufang/'
url_dict['嘉定']='https://sh.58.com/jiading/zufang/'
url_dict['徐汇']='https://sh.58.com/xuhui/zufang/'
url_dict['静安']='https://sh.58.com/jingan/zufang/'
url_dict['普陀']='https://sh.58.com/putuo/zufang/'
url_dict['杨浦']='https://sh.58.com/yangpu/zufang/'
url_dict['奉贤']='https://sh.58.com/fengxiansh/zufang/'
url_dict['黄浦']='https://sh.58.com/huangpu/zufang/'
url_dict['虹口']='https://sh.58.com/hongkou/zufang/'
url_dict['长宁']='https://sh.58.com/changning/zufang/'
url_dict['宝安']='https://sz.58.com/baoan/zufang/'
url_dict['南山']='https://sz.58.com/nanshan/zufang/'
url_dict['龙华']='https://sz.58.com/szlhxq/zufang/'
url_dict['福田']='https://sz.58.com/futian/zufang/'
url_dict['龙岗']='https://sz.58.com/longgang/zufang/'
url_dict['罗湖']='https://sz.58.com/luohu/zufang/'
url_dict['布吉']='https://sz.58.com/buji/zufang/'
url_dict['光明']='https://sz.58.com/guangmingxinqu/zufang/'
url_dict['盐田']='https://sz.58.com/yantian/zufang/'

USER_AGENT_LIST=[
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/22.0.1207.1 Safari/537.1",
    "Mozilla/5.0 (X11; CrOS i686 2268.111.0) AppleWebKit/536.11 (KHTML, like Gecko) Chrome/20.0.1132.57 Safari/536.11",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1092.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.6 (KHTML, like Gecko) Chrome/20.0.1090.0 Safari/536.6",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/19.77.34.5 Safari/537.1",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.9 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.0) AppleWebKit/536.5 (KHTML, like Gecko) Chrome/19.0.1084.36 Safari/536.5",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 5.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1063.0 Safari/536.3",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; Trident/4.0; SE 2.X MetaSr 1.0; SE 2.X MetaSr 1.0; .NET CLR 2.0.50727; SE 2.X MetaSr 1.0)",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1062.0 Safari/536.3",
    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; 360SE)",
    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.1 Safari/536.3",
    "Mozilla/5.0 (Windows NT 6.2) AppleWebKit/536.3 (KHTML, like Gecko) Chrome/19.0.1061.0 Safari/536.3",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24",
    "Mozilla/5.0 (Windows NT 6.2; WOW64) AppleWebKit/535.24 (KHTML, like Gecko) Chrome/19.0.1055.1 Safari/535.24"
]





myheaders = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Encoding": "br, gzip, deflate",
    "Accept-Language": "zh-cn",
    "User-Agent": "NULL"
  }

def get_proxy(new_url):
    apis = [
        "http://120.79.85.144/index.php/api/entry?method=proxyServer.tiqu_api_url&packid=0&fa=0&groupid=0&fetch_key=&qty=30&time=1&port=1&format=txt&ss=1&css=&pro=&city=&dt=0&usertype=17"
        "http://120.79.85.144/index.php/api/entry?method=proxyServer.tiqu_api_url&packid=0&fa=0&groupid=0&fetch_key=&qty=15&time=1&port=1&format=txt&ss=1&css=&pro=&city=&dt=0&usertype=17"
    ]
    for i in range(10000):
        response_ = requests.get(np.random.choice(apis))
        print(response_.text)
        proxy_list=response_.text.split()
        proxies = {
            "http": "123.73.82.63:32223",
            "https": "123.73.82.63:32223"
        }
        for proxy in proxy_list:
            proxies["http"] = "http://" + proxy
            proxies["https"] = "http://" + proxy
            try:
                print(new_url)
                response = requests.get(new_url, proxies=proxies,timeout=10)
                #print(response.text)
                if (response.status_code == 200):  # 检查是否可用
                    print("连接成功了！")
                else:
                    print(response.text)
                return response.text
            except:
                print("当前IP不可用")
        time.sleep(3)




def get_index():#使用Selenium从网页获得检索结果
    for city in geo:  # 城市
        df = pd.DataFrame(all_data, columns=['租房网站名称', '城市', '区', 'link', '信息发布人', '是否有详细信息'])
        df.drop_duplicates()  # 去除重复元素
        df.to_excel('./webdatamining/house_rent_tmp.xlsx', index=False, header=True)
        for district in geo[city]:  # 返回区的名称
            base_url = url_dict[district]
            for i in range(10):  # 爬取前10页的内容
                new_url=base_url+"pn"+str(i+1)+"/"
                print("第" + str(i + 1) + "页")
                #response=requests.get(new_url)
                response=get_proxy(new_url)
                soup = BeautifulSoup(response, 'html.parser')
                for item in soup.find_all('li', class_='house-cell'):
                    house_data = []  # 该房子的信息
                    house_data.append("58同城")  # 网站名
                    house_data.append(city)  # 城市
                    house_data.append(district)  # 区
                    info = item.find('div', class_='des')
                    link = info.h2.find('a', class_='strongbox')['href']
                    print(link)
                    house_data.append(link)  # 链接
                    uploader = info.find_all('div', class_='jjr')
                    if (len(uploader) > 0):
                        uploader_type = "经纪人"
                    else:
                        uploader_type = "个人"
                    house_data.append(uploader_type)
                    house_data.append(0)
                    all_data.append(house_data)  # 加入总列表
                time.sleep(5)
                time.sleep(5)
            time.sleep(5)
    df = pd.DataFrame(all_data, columns=['租房网站名称', '城市', '区', 'link', '信息发布人','是否有详细信息'])
    df.drop_duplicates()#去除重复元素
    df.to_excel('./webdatamining/house_rent.xlsx', index=False, header=True)


def get_detailed():
    df=pd.read_excel("./webdatamining/house_rent_full44.0.xlsx",header=0)
    df_copy=df.copy(deep=True)#拷贝一份
    df2=pd.DataFrame()
    for index,row in df_copy.iterrows():
        try:
            if (int(row['是否有详细信息']) != 0):
                continue  # 已经爬取
            if (index<=4400):
                continue
            time.sleep(5)
            myheaders["User-Agent"] = np.random.choice(USER_AGENT_LIST)
            base_url = row['link']  # 取出链接
            print("crawling:", base_url)
            text = get_proxy(base_url)
            if (index % 100 == 0):  # 中途保存
                df.to_excel('./webdatamining/house_rent_full' + str(index / 100) + '.xlsx', index=False, header=True)
            # 获取加密字符串

            bs64_str = re.findall("charset=utf-8;base64,(.*?)'\)", text)[0]
            df2.loc[index, '加密'] = bs64_str
            b = base64.b64decode(bs64_str)
            font = TTFont(io.BytesIO(b))
            bestcmap = font['cmap'].getBestCmap()
            newmap = dict()
            for key in bestcmap.keys():
                value = int(re.search(r'(\d+)', bestcmap[key]).group(1)) - 1
                key = hex(key)
                newmap[key] = value
            # 把页面上自定义字体替换成正常字体
            response_ = text
            for key, value in newmap.items():
                key_ = key.replace('0x', '&#x') + ';'
                if key_ in response_:
                    response_ = response_.replace(key_, str(value))
            soup = BeautifulSoup(response_, 'html.parser')

            #soup=BeautifulSoup(text,'html.parser')
            time.sleep(10)
            for item in soup.find_all('div', class_='house-basic-desc'):
                # print("ITEM", item)
                x1 = item.find('div', class_='house-pay-way')
                x2 = x1.find('span')
                price = x2.b.get_text()
                print("租金:", price)
                df.loc[index, '租金'] = price  # rent
                list = item.find('ul', class_='f14')
                list_of_content = list.find_all('li')
                ##户型
                strs = list_of_content[1].find_all('span')[1].get_text().strip()
                strs = strs.split()
                shi = strs[0][0]
                ting = strs[0][2]
                wei = strs[0][4]
                df.loc['户型'] = strs[0]
                ##面积
                area = strs[1]
                print("户型", shi, ting, wei, "面积", area)
                df.loc[index, '室'] = shi
                df.loc[index, '厅'] = ting
                df.loc[index, '卫'] = wei
                df.loc[index, '面积'] = area
                ##朝向楼层
                strs = list_of_content[2].find_all('span')[1].get_text().strip()
                strs = strs.split()
                direction = strs[0]
                floor = strs[1][:-1]
                total_floor = strs[3][:-1]
                print("朝向", direction, "总楼层", total_floor, "楼层", floor)
                df.loc[index, '朝向'] = direction
                df.loc[index, '总楼层'] = total_floor
                df.loc[index, '所属楼层'] = floor
                ##小区
                xq = list_of_content[3].find_all('span')[1].get_text().strip()
                print("小区名", xq)
                df.loc[index, '小区'] = xq
                ##详细地址
                add = list_of_content[-1].find_all('span')[1].get_text().strip()
                print("详细地址", add)
                df.loc[index, '详细地址'] = add
            for item in soup.find_all('div', class_='house-detail-desc'):
                lists = item.find_all('ul', class_='house-disposal')
                flag = False
                for list in lists:
                    balcony = list.find_all('li', class_='balcony')
                    if (len(balcony) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有阳台'] = 1
                else:
                    df.loc[index, '是否有阳台'] = 0

                flag = False
                for list in lists:
                    bed = list.find_all('li', class_='bed')
                    if (len(bed) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有床'] = 1
                else:
                    df.loc[index, '是否有床'] = 0

                flag = False
                for list in lists:
                    chest = list.find_all('li', class_='chest')
                    if (len(chest) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有衣柜'] = 1
                else:
                    df.loc[index, '是否有衣柜'] = 0

                flag = False
                for list in lists:
                    sofa = list.find_all('li', class_='sofa')
                    if (len(sofa) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有沙发'] = 1
                else:
                    df.loc[index, '是否有沙发'] = 0

                flag = False
                for list in lists:
                    tv = list.find_all('li', class_='telev')
                    if (len(tv) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有电视'] = 1
                else:
                    df.loc[index, '是否有电视'] = 0

                flag = False
                for list in lists:
                    fridge = list.find_all('li', class_='icebox')
                    if (len(fridge) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有冰箱'] = 1
                else:
                    df.loc[index, '是否有冰箱'] = 0

                flag = False
                for list in lists:
                    washer = list.find_all('li', class_='washer')
                    if (len(washer) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有洗衣机'] = 1
                else:
                    df.loc[index, '是否有洗衣机'] = 0

                flag = False
                for list in lists:
                    kt = list.find_all('li', class_='air-condition')
                    if (len(kt) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有空调'] = 1
                else:
                    df.loc[index, '是否有空调'] = 0

                flag = False
                for list in lists:
                    ht = list.find_all('li', class_='central-heater')
                    if (len(ht) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有暖气'] = 1
                else:
                    df.loc[index, '是否有暖气'] = 0
                flag = False
                for list in lists:
                    stove = list.find_all('li', class_='fuel-gas')
                    if (len(stove) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有燃气'] = 1
                else:
                    df.loc[index, '是否有燃气'] = 0

                flag = False
                for list in lists:
                    water = list.find_all('li', class_='water-heater')
                    if (len(water) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有热水器'] = 1
                else:
                    df.loc[index, '是否有热水器'] = 0
                flag = False
                for list in lists:
                    net = list.find_all('li', class_='broadband')
                    if (len(net) > 0):
                        flag = True
                if (flag):
                    df.loc[index, '是否有宽带'] = 1
                else:
                    df.loc[index, '是否有宽带'] = 0
                print(df.loc[index, :])
            time.sleep(5)
            df.loc[index, '是否有详细信息'] = 1
        except Exception as err:
            print(err)
            continue
    df.to_excel('./webdatamining/house_rent_full.xlsx', index=False, header=True)
    df2.to_excel('./webdatamining/code_base64.xlsx',index=False,header=True)


def make_font_file( base64_string: str):
    bin_data = base64.decodebytes(base64_string.encode())
    return bin_data

def get_num(string, c_list):
    """
    我们根据网页抓取的乱码的unicode编码，获取其对对应的字源，即可获取所对应的数字
    :param string:
    :return:
    """
    ret_list = []
    for char in string:
        decode_num = ord(char)
        num = c_list[decode_num]
        num = int(num[-2:]) - 1
        ret_list.append(num)
    return ret_list

def decode_str(code,text):
    #base64_string = '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'
    #text='龒鑶龥麣'
    font = TTFont(BytesIO(make_font_file(code)))
    code_list = font['cmap'].tables[0].ttFont.tables['cmap'].tables[0].cmap
    crack_text=''.join([str(i) for i in get_num(text,code_list)])
    print(crack_text)
    return (int(crack_text))



#get_index()
get_detailed()

